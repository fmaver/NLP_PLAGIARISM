{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LAB 1 \n",
    "Este primer laboratorio le presenta NLTK y lo lleva a través de los métodos de la biblioteca para acceder a los Corpus y trabajar con texto. En particular, este laboratorio se centra en calcular el recuento de palabras de un corpus."
   ],
   "id": "52d87ed13513f418"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### NLTK Library\n",
    "If you have not installed the NLTK library, you can do so by running the following command in the teriminal:\n",
    "```bash\n",
    "poetry add nltk\n",
    "```"
   ],
   "id": "ceeefcd1f3f6ad04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:00:09.099307Z",
     "start_time": "2024-06-08T18:53:39.186937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ],
   "id": "614dbc67bddb5514",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:02:19.046953Z",
     "start_time": "2024-06-08T19:02:19.044483Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.book import *",
   "id": "49831a38ae03cf7b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:04:32.266718Z",
     "start_time": "2024-06-08T19:04:32.259742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(text1.tokens[0:5])\n",
    "print(f\"Moby occurrences: {text1.count(\"Moby\")}\")\n",
    "print(text7.tokens[0:5])\n",
    "print(\"\\n\")\n",
    "print(texts())"
   ],
   "id": "f06014a3ad9abd51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman']\n",
      "Moby occurrences: 84\n",
      "['Pierre', 'Vinken', ',', '61', 'years']\n",
      "\n",
      "\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "word_tokenize\tes\tun\tmétodo\tbuilt-in\tque\t“tokeniza”\tuna\tcadena\tde\tlenguaje\tnatural.\tNotese\tque\tla\ttokenización\tno\testá\tbasada\tsolamente\ten\tseparar\tpor\tespacios.\t\tlen(mysent_tokens)\tretorna\tel\ttamaño\tde\tla\tlista\tde\ttokens\tdado\tcomo\targumento\t\t",
   "id": "b7b093aac0a20116"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:08:38.174981Z",
     "start_time": "2024-06-08T19:08:38.171896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "mysent = \"The cat sat on the mat.\"\n",
    "mysent_tokens = word_tokenize(mysent)\n",
    "print(mysent_tokens)\n",
    "print(f\"Sentences Length: {len(mysent_tokens)}\")"
   ],
   "id": "5ac628f75a81e40e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
      "Sentences Length: 7\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:10:57.722284Z",
     "start_time": "2024-06-08T19:10:57.719026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import re\n",
    "m = re.search(\"gua\", \"natural language processing\")\n",
    "print(m)\n",
    "print(f\"The pattern appears in the string at index {m.start()} and ends at index {m.end()}\\n\")\n",
    "\n",
    "n = re.search(\"guags\", \"processing natural language\")\n",
    "print(n)"
   ],
   "id": "f9c6eb75c99af999",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(11, 14), match='gua'>\n",
      "The pattern appears in the string at index 11 and ends at index 14\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora supongamos que consideramos como palabras, solo aquellos tokens que son alfanuméricos, es decir. solo contienen alfabetos, números o el hiper símbolos. La expresión regular que puede usar para este patrón es \"\\ w\"",
   "id": "5daa18973b2fb248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:18:30.819680Z",
     "start_time": "2024-06-08T19:18:30.816257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mysent_tokens_not_punct = [word for word in word_tokenize(mysent.lower()) if re.search(\"\\w\", word)]\n",
    "print(mysent_tokens_not_punct)\n",
    "print(f\"Sentences Length: {len(mysent_tokens_not_punct)}\\n\")\n",
    "print(set(mysent_tokens_not_punct))\n",
    "print(f\"Unique Words Size: {len(set(mysent_tokens_not_punct))}\")"
   ],
   "id": "22222c789bfe7370",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Sentences Length: 6\n",
      "\n",
      "{'sat', 'mat', 'the', 'cat', 'on'}\n",
      "Unique Words Size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/n7/q7zcbzwd06x1qywn2j98x0y40000gn/T/ipykernel_2164/1920290807.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  mysent_tokens_not_punct = [word for word in word_tokenize(mysent.lower()) if re.search(\"\\w\", word)]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PROBANDO CONOCIMIENTO\n",
    "types = cantidad unica de palabras"
   ],
   "id": "db51e9d14d90a06c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.\tCuál\tes\tel\tnúmero\tde\ttokens\ten\tMoby\tDick?\t",
   "id": "9cb2d21f2902a38a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:21:27.892561Z",
     "start_time": "2024-06-08T19:21:27.765499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "moby_dick_token = text1.tokens\n",
    "moby_dick_token_not_punct = [word.lower() for word in moby_dick_token if re.search(\"\\w\", word)]\n",
    "print(f\"Sentences Length: {len(moby_dick_token_not_punct)}\")\n",
    "print(moby_dick_token_not_punct[0:5])"
   ],
   "id": "78f0e3a7287891a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Length: 218621\n",
      "['moby', 'dick', 'by', 'herman', 'melville']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/n7/q7zcbzwd06x1qywn2j98x0y40000gn/T/ipykernel_2164/824518874.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  moby_dick_token_not_punct = [word.lower() for word in moby_dick_token if re.search(\"\\w\", word)]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.\tCuál\tes\tel\tnúmero\tde\ttypes\ten\tMoby\tDick?\t\t",
   "id": "fc645fa87cfcdad7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:24:55.977590Z",
     "start_time": "2024-06-08T19:24:55.966735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_moby_dick_words = set(moby_dick_token_not_punct)\n",
    "print(f\"Type: {len(unique_moby_dick_words)}\")"
   ],
   "id": "c9322c6325dec80e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: 17140\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "El\ttype-token\tratio\tes\tuna\tmedida\tde\tcuán\tdiverso\tson\tlos\títems\tléxicos\ten\tun\ttexto\tdado.\tEstá\tdefinido\tpor\tel\tnúmero\tde\ttypes\tdividido\tpor\tel\tnúmero\tde\ttokens.\tCuánto\tmás\talto\tes\teste\tratio,\tconsideramos\tal\ttexto\tcomo\tmás\tdiverso\ten\tpalabras,\ttambién\tconocido\tcomo\t“diversidad\tléxica”.\t",
   "id": "d82be7115269e0d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:28:25.965398Z",
     "start_time": "2024-06-08T19:28:25.961691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "moby_dick_type_token_ratio = len(unique_moby_dick_words)/len(moby_dick_token_not_punct)\n",
    "print(f\"Moby Dick Type Token Ratio: {moby_dick_type_token_ratio}\")\n",
    "print(f\"In percentage: {round(moby_dick_type_token_ratio*100, 2)}%\")\n",
    "\n",
    "# The result make senses due to the huge amount of tokens"
   ],
   "id": "d0da1085c4c8c10f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby Dick Type Token Ratio: 0.07840051962071347\n",
      "In percentage: 7.84%\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "WALL STREET JOURNAL",
   "id": "c17bfc67a75c04a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:31:00.722887Z",
     "start_time": "2024-06-08T19:31:00.663955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wall_street_token = text7.tokens\n",
    "wall_street_token_not_punct = [word.lower() for word in wall_street_token if re.search(\"\\w\", word)]\n",
    "print(f\"Sentences Length: {len(wall_street_token_not_punct)}\")\n",
    "\n",
    "unique_wall_street_words = set(wall_street_token_not_punct)\n",
    "print(f\"Type: {len(unique_wall_street_words)}\")\n",
    "\n",
    "wall_street_type_token_ratio = len(unique_wall_street_words)/len(wall_street_token_not_punct)\n",
    "print(f\"Wall Street Type Token Ratio: {wall_street_type_token_ratio}\")\n",
    "print(f\"In percentage: {round(wall_street_type_token_ratio*100, 2)}%\")"
   ],
   "id": "464fa71ea2ac0b4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Length: 87608\n",
      "Type: 11367\n",
      "Moby Dick Type Token Ratio: 0.129748424801388\n",
      "In percentage: 12.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/n7/q7zcbzwd06x1qywn2j98x0y40000gn/T/ipykernel_2164/1219788111.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  wall_street_token_not_punct = [word.lower() for word in wall_street_token if re.search(\"\\w\", word)]\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that the Wall Street Book has more lexical diversity. The main reason is that this book is much shorter than the Moby Dick one, meaning it has fewer tokens. And normally, a shorter book will have a bigger TTR (type-token-ratio) than a bigger one. More info here:\n",
    "[token-radio](https://lexically.net/downloads/version7/HTML/type_token_ratio_proc.htm)"
   ],
   "id": "7e2f89dec4b23ca4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Cual\tes\tel\t“Maximum\tLikelikhood\tEstimate\t(MLE)”\tde\tla\tpalabra\t“whale”\t(ballena)\ten\tMoby\tDick?\n",
    "* P(Moby_dick(\"Whale\")\t"
   ],
   "id": "ada3f33a125222c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:48:33.751733Z",
     "start_time": "2024-06-08T19:48:33.749650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate Maximum\tLikelikhood\tEstimate (MLE) from the word “whale” in Moby Dick?\n",
    "\n",
    "\n"
   ],
   "id": "f1a27ce33265ab7a",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ac3a704ba985b9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
